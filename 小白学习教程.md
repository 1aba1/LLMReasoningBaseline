## 项目小白学习教程（Direct + CoT Baseline）

> 面向零基础同学的循序渐进教程：从“这个项目在干嘛”到“我能自己加一点功能”。建议你从头到尾读一遍，然后按练手任务一步步来。

---

### 一、这个项目是在干嘛？

- **项目目标**
  - 给一组题目/样本（通常是 JSON/JSONL 数据集），用大模型执行一种“推理范式”（目前是 Direct），得到模型输出。
  - 对全量样本计算**准确率**、**token 消耗**等指标，作为一个“基线实验（baseline）”。

- **当前已实现**
  - **模型层**：使用 OpenAI / OpenAI 兼容 API（通过 `openai` 官方 SDK）。
  - **方法层**：`Direct` 方法 —— 即直接让模型输出答案（不显式要求写思维链 CoT）。

- **未来可以扩展**
  - 在 `src/methods` 新增 `cot.py`，实现“思维链 CoT”方法。
  - 在 `src/llms` 新增其它模型适配器，比如自建大模型服务，只要实现统一的 `BaseLLM` 接口即可。

你可以把整个项目理解为：

> **给定：配置 + 数据集 + prompt 模板 + 大模型** → 运行一次实验 → 输出：日志 + 指标。

---

### 二、整体结构一眼看懂

这里只介绍跟你最相关的几个文件/目录（路径都是相对项目根目录）：

- **`run.py`**
  - 整个项目的**入口脚本**，你运行的就是它：`python run.py`。
  - 负责：读取配置、初始化模型与方法、加载数据、循环跑样本、写日志、算指标。

- **`src/llms/openai_client.py`**
  - OpenAI / OpenAI 兼容 API 的**客户端适配器**。
  - 上层只管调用 `llm.generate(prompt)`，底层由它去调用真正的 OpenAI 接口。

- **`src/methods/direct.py`**（你可以自己打开看）
  - Direct 方法实现：如何根据 `sample` + prompt 模板构造 prompt，如何解析模型输出，如何判断对错。

- **`configs/` 目录**（典型结构，具体文件名以你项目为准）
  - `configs/run_config.yaml`：一次实验的“总配置入口”，里面会引用：
    - 模型配置文件（例如 `configs/model/openai_gpt-4.yaml`）
    - 方法配置文件（例如 `configs/method/direct.yaml`）
    - 数据集路径、prompt 文件路径、输出目录等。
  - 其它 YAML 文件：细分的模型配置、方法配置等。

- **`data/` 目录**
  - 存放数据集，比如某个 JSON / JSONL 文件。

- **`outputs/` 目录**
  - 每次运行 `python run.py` 后，会在这里生成一个新的时间戳文件夹：
    - `full_log.jsonl`：每条样本的详细结果（一行一个 JSON）。
    - `metrics.json`：本次实验的整体指标（准确率、token 用量等）。

---

### 三、深入理解 `run.py`：一次实验是怎么跑起来的？

`run.py` 是你理解整个项目的“入口地图”。它的大致步骤如下。

#### 1. 路径与工具函数

- **`ROOT_DIR`**
  - 表示项目根目录（即 `run.py` 所在目录）。

- **`load_yaml(path)`**
  - 功能：读取一个 YAML 配置文件，返回 Python 字典。
  - 用途：
    - 读取运行总配置 `configs/run_config.yaml`。
    - 读取模型配置 `model_config.yaml`。
    - 读取方法配置 `method_config.yaml`。

- **`read_text(path)`**
  - 功能：读取一个纯文本文件（例如 prompt 模板），返回字符串。

- **`resolve_path(relative)`**
  - 功能：把配置文件里写的“相对路径”转成“绝对路径”。
  - 好处：你无论在什么工作目录执行 `python run.py`，都能正确找到文件路径，不会因为当前目录不同而出错。

#### 2. `main()` 函数整体流程

`main()` 是整个实验的**主流程**，主要分为八步：

1. **加载环境变量**
   - 调用 `load_dotenv()`，从项目根目录的 `.env` 文件读取环境变量。
   - 比如 `OPENAI_API_KEY`、`OPENAI_BASE_URL` 等配置。

2. **读取运行总配置 `run_config.yaml`**
   - 从 `configs/run_config.yaml` 得到本次实验需要的所有“入口信息”，包括：
     - `model_config`：模型配置文件路径。
     - `method_config`：方法配置文件路径。
     - `input_file`：数据集路径。
     - `prompt_file`：prompt 模板路径。
     - `output_dir`：输出根目录。
     - `max_samples`：最多使用多少条样本（可选）。
   - 接着用 `load_yaml` / `resolve_path` 把这些配置文件和路径都加载/转成绝对路径。

3. **初始化模型 `OpenAIClient`**
   - 通过 `model_config.yaml` 读取：
     - `model_type`（当前必须是 `openai`）
     - `model_name`（如 `gpt-4o`、`gpt-4.1` 等）
     - `base_url`（可选，适配兼容 API）
     - `max_tokens`、`temperature` 等默认生成参数。
   - 若 `model_type` 不是 `openai`，会直接抛出 `NotImplementedError`。
   - 用这些参数构造 `OpenAIClient` 实例：
     - 保存为 `llm` 变量，后续方法层统一使用这个对象。

4. **初始化方法（解题器） `DirectSolver`**
   - 从 `method_config.yaml` 中读取 `method_name`（当前必须是 `direct`）。
   - 用 `read_text(prompt_file)` 读入 prompt 模板文本。
   - 构造 `DirectSolver(llm=llm, prompt_template=prompt_template)`：
     - 其中 `llm` 是刚才构造的模型客户端。
     - `prompt_template` 是文本模板，将用来拼接每个样本的 prompt。

5. **加载数据集**
   - 使用 `load_json_dataset(input_file, max_samples=max_samples)`：
     - 读入 JSON/JSONL 格式的数据集。
     - 得到一个 Python 列表 `dataset`，其中每个元素是一个 `sample` 字典。
     - 典型字段可能包含：`id`、`question`、`options`、`answer` 等。

6. **为本次运行创建输出目录**
   - 生成一个时间戳字符串 `timestamp`（如 `20251216_101530`）。
   - 从配置中读取：`run_name`、`method_name`、`model_name`。
   - 拼出一个唯一的文件夹名，例如：
     - `20251216_101530_direct_baseline_direct_gpt-4o`
   - 创建该文件夹，所有日志和指标都放在里面：
     - `full_log.jsonl`：逐条样本结果日志。
     - `metrics.json`：整体指标汇总。

7. **主循环：对每个样本调用 LLM**
   - 遍历 `dataset` 中的每个 `sample`：
     - 打印当前进度（第几个样本、样本 id）。
     - 调用 `solver.run_sample(sample)`：
       - 内部会使用 `llm.generate(prompt)` 调用大模型。
       - 会解析模型输出，得到预测答案、对错信息，以及 token 用量。
     - 把每条 `result`：
       - 放入 `records` 列表，用于后续计算指标。
       - 以一行 JSON 的形式写入 `full_log.jsonl`。

8. **计算整体指标并保存**
   - `accuracy = compute_accuracy(records)`：
     - 根据每条 `result` 里的 `is_correct`，算出整体准确率。
   - `token_usage = aggregate_token_usage(records)`：
     - 累加整个数据集上的 `prompt_tokens`、`completion_tokens`、`total_tokens`。
   - 把这些打包成一个 `metrics` 字典，写入 `metrics.json`，并在终端打印。

到这里，你应该对 `run.py` 的角色有一个清晰的认识：

> **它是“实验总控脚本”，不做具体的模型细节和方法细节，只负责组织流程。**

---

### 四、模型适配层：理解 `OpenAIClient`

文件：`src/llms/openai_client.py`

这个类的作用是：

> 把你项目里统一的“大模型接口”(`BaseLLM`) 和 OpenAI 官方 SDK 对接起来。

#### 1. 继承与统一接口

- `OpenAIClient` 继承自 `BaseLLM`：

  ```python
  from src.core.llm_base import BaseLLM, LLMResult

  class OpenAIClient(BaseLLM):
      ...
  ```

- `BaseLLM` 统一约定了：
  - 上层只关心有一个 `generate(prompt: str, **kwargs) -> LLMResult` 方法即可。
  - 这样，不同的模型（OpenAI、本地模型、第三方服务）都可以通过实现 `BaseLLM` 的接口来接入。

- `LLMResult` 是一次模型调用的标准返回结构，典型包含：
  - `text`：生成的文本内容。
  - `prompt_tokens` / `completion_tokens` / `total_tokens`：token 用量（可选）。
  - `raw`：底层原始响应，便于调试和深度分析。

#### 2. 初始化：`__init__`

主要功能：

1. **加载环境变量**
   - 通过 `load_dotenv()` 从 `.env` 文件加载环境变量。
   - 必须保证 `.env` 中配置了 `OPENAI_API_KEY`。

2. **读取 API Key 和 Base URL**
   - 通过 `os.getenv("OPENAI_API_KEY")` 获取 API Key，如果没有就抛出异常提醒你配置。
   - 通过：
     - 构造函数参数 `base_url`，
     - 或 `.env` 中的 `OPENAI_BASE_URL`，
     - 来支持 OpenAI 兼容 API（自己部署的服务等）。

3. **创建 OpenAI 客户端实例**

   ```python
   self.client = OpenAI(api_key=api_key, base_url=base_url)
   ```

4. **保存默认参数**
   - `self.model_name`：模型名称。
   - `self.max_tokens`：默认生成最大 token 数。
   - `self.temperature`：默认温度（控制随机性）。

#### 3. 统一的生成方法：`generate(prompt, **kwargs)`

核心逻辑：

1. **支持覆盖默认参数**

   ```python
   max_tokens = kwargs.get("max_tokens", self.max_tokens)
   temperature = kwargs.get("temperature", self.temperature)
   ```

2. **调用 OpenAI Chat API**

   ```python
   response = self.client.chat.completions.create(
       model=self.model_name,
       messages=[
           {"role": "system", "content": "You are a helpful assistant."},
           {"role": "user", "content": prompt},
       ],
       max_tokens=max_tokens,
       temperature=temperature,
   )
   ```

   - 把你给的 `prompt` 放到 `user` 角色里。
   - `system` 里写一条简单说明："You are a helpful assistant."。

3. **提取内容和 token 用量**
   - 从 `response.choices[0].message.content` 中取出生成文本。
   - 从 `response.usage` 中取出 `prompt_tokens` / `completion_tokens` / `total_tokens`（如果存在）。

4. **封装为 `LLMResult` 返回**

   ```python
   return LLMResult(
       text=text,
       prompt_tokens=prompt_tokens,
       completion_tokens=completion_tokens,
       total_tokens=total_tokens,
       raw=response.to_dict() if hasattr(response, "to_dict") else dict(response),
   )
   ```

> 对于上层（例如 `DirectSolver`）来说，只需要：`llm.generate(prompt)` ➜ 得到一个包含 `text` 和 token 信息的 `LLMResult`。

---

### 五、方法层（推理范式）：以 Direct 为例

虽然你当前只看到 `run.py` 引用了 `DirectSolver`，没完全展开它的源码，这里给你一个**典型 Direct 方法实现思路**，帮助理解：

1. **构造 prompt**：
   - 从 `sample` 里拿到题干、选项等信息。
   - 用 prompt 模板（例如 `configs/prompts/direct.txt`）进行字符串格式化。

2. **调用模型**：
   - 把构造好的 `prompt` 交给 `llm.generate(prompt)`。
   - 得到一次调用的 `LLMResult`。

3. **解析答案**：
   - 从 `LLMResult.text` 中抽取真正的答案（例如选项字母 A/B/C/D）。

4. **评估结果**：
   - 把解析出的答案和 `sample` 的标准答案对比，得到对错信息。

5. **打包记录**：
   - 返回一个结构化的字典结果：包含 id、prompt、模型原始输出、预测答案、标准答案、是否正确、token 用量等。

---

### 六、【强烈建议】小白循序渐进练手路线

下面是为你设计的练手路径，从最简单的“改打印语句”到“新增一个 CoT 方法”。你可以一项一项来。

#### 练习 1：改打印日志（熟悉主流程）

**目标**：不改变逻辑，只改变 `run.py` 的打印信息，让你熟悉主循环。

1. 打开 `run.py`，找到主循环里的这行：

   ```python
   print(f"处理样本 {i}/{len(dataset)} (id={sample.get('id')}) ...")
   ```

2. 改成更详细一点，比如：

   ```python
   from datetime import datetime

   print(f"[{datetime.now()}] 正在处理第 {i} / {len(dataset)} 个样本, id={sample.get('id')}")
   ```

3. 重新运行 `python run.py`，查看终端输出是否发生了变化。

通过这个练习，你会：
- 学会如何定位主循环。
- 理解每个 `sample` 是如何被遍历处理的。

#### 练习 2：改模型默认参数（熟悉模型适配层）

**目标**：调整 `temperature` 和 `max_tokens`，感受结果和 token 用量变化。

**做法一：通过配置文件修改（推荐）**

1. 打开 `configs/model/...yaml`（具体文件名以 `run_config.yaml` 中 `model_config` 为准）。
2. 找到类似配置：

   ```yaml
   temperature: 0.0
   max_tokens: 512
   ```

3. 改成：

   ```yaml
   temperature: 0.7
   max_tokens: 256
   ```

4. 保存并重新运行 `python run.py`，然后：
   - 比较模型输出内容是否更发散、更“话多”。
   - 比较 `metrics.json` 中 `token_usage` 是否发生变化。

**做法二：在代码中临时覆盖**

1. 打开 `run.py`，找到初始化 `OpenAIClient` 的地方：

   ```python
   llm = OpenAIClient(
       model_name=model_cfg.get("model_name"),
       base_url=model_cfg.get("base_url"),
       max_tokens=model_cfg.get("max_tokens", 512),
       temperature=model_cfg.get("temperature", 0.0),
   )
   ```

2. 把里面的值直接改成你想要的：

   ```python
   llm = OpenAIClient(
       model_name=model_cfg.get("model_name"),
       base_url=model_cfg.get("base_url"),
       max_tokens=256,
       temperature=0.7,
   )
   ```

通过这个练习，你会：
- 理解“配置 → 模型客户端参数 → 实际 API 调用”的链路。

#### 练习 3：修改 Direct 的 prompt 模板（熟悉方法层）

**目标**：感受 prompt 改动对模型行为的影响。

1. 打开 `configs/run_config.yaml`，找到 `prompt_file` 对应的路径，比如：

   ```yaml
   prompt_file: configs/prompts/direct.txt
   ```

2. 打开这个 prompt 模板文件，在末尾增加一句中文说明，例如：

   ```text
   请你一步一步地分析题目，并在最后一行给出你的答案。
   ```

3. 保存后重新运行 `python run.py`，然后：
   - 打开这次新的输出目录下的 `full_log.jsonl`。
   - 查看模型的输出中，是否开始多写一些“推理过程”。
   - 查看 `metrics.json` 中 `completion_tokens` 是否变多。

通过这个练习，你会：
- 理解“prompt 模板 → 模型行为”的关系。
- 学会如何通过修改 prompt 来控制模型风格。

#### 练习 4：新增一个最简单的 CoT 方法（进阶）

当你对上面 3 个练习比较熟悉后，就可以尝试“正式扩展”项目了。

**目标**：复制 Direct 方法，新建一个 `CoTSolver`，强制模型输出思维链。

> 以下步骤是一个“通用思路”，文件名/类名请根据你实际项目稍作调整。

1. 在 `src/methods` 中复制 `direct.py` 为 `cot.py`。
2. 打开 `cot.py`，把类名从 `DirectSolver` 改成 `CoTSolver`。
3. 在 `__init__` / `build_prompt` 中，确保 prompt 中有显式要求：

   ```text
   请先写出详细的推理过程，然后在最后一行以“答案：X”的格式给出你的最终答案。
   ```

4. 修改 `parse_answer`（或类似函数）：
   - 从模型完整输出中，取出最后一行，截取 `答案：` 后面的内容，作为 `pred_answer`。

5. 新建一个方法配置文件，例如 `configs/method/cot.yaml`：

   ```yaml
   method_name: cot
   # 其它需要的参数，比如 prompt 模板路径等（如果有的话）
   ```

6. 修改 `run_config.yaml`（或复制一份新的运行配置）：

   ```yaml
   method_config: configs/method/cot.yaml
   run_name: cot_baseline
   ```

7. 根据需要，准备一个新的 prompt 模板（比如 `configs/prompts/cot.txt`），并在 `run_config.yaml` 中把 `prompt_file` 指向它。

8. 运行 `python run.py`，比较：
   - CoT 方法和 Direct 方法在准确率上的差别。
   - token 用量上的差别（一般 CoT 会多一些）。

通过这个练习，你会：
- 体验“在既有框架下新增一种方法”的完整流程。
- 真正理解“推理范式（Direct / CoT）”在代码层是怎么被抽象出来的。

---

### 七、常见问题与排查小贴士

- **Q1：运行时报错 `OPENAI_API_KEY 未配置` 怎么办？**
  - 在项目根目录创建/编辑 `.env` 文件，加入：

    ```text
    OPENAI_API_KEY=你的Key
    # 如果你使用 OpenAI 兼容服务，还可以配置：
    # OPENAI_BASE_URL=https://你的服务地址/v1
    ```

  - 保存后重新运行 `python run.py`。

- **Q2：路径相关错误（文件找不到）怎么办？**
  - 优先检查 `configs/run_config.yaml` 里的路径是否正确。
  - 确认这些路径是相对于项目根目录的。
  - 代码内部会通过 `resolve_path` 转成绝对路径，一般不会出错，只要配置无误。

- **Q3：想只跑一小部分数据试试？**
  - 在 `configs/run_config.yaml` 里设置：

    ```yaml
    max_samples: 20
    ```

  - 或者改成别的更小的数值，便于快速迭代与调试。

---

### 八、下一步我可以怎么帮你？

如果你读完这个教程，推荐你接下来做这三件事：

1. **按“练习 1～3” 动手改一轮**，跑几次 `python run.py`，观察输出和指标变化。
2. 如果你愿意挑战一下，就试着完成“练习 4：新增 CoT 方法雏形”。
3. 遇到任何具体代码（例如某个函数）看不懂，可以把那一段贴给我，让我给你做“逐行中文讲解 + 小练习设计”。

只要你跟着这些练习慢慢来，很快就能在这个项目上自行修改、添加功能，而不是只能“黑盒”运行。祝你玩得开心、学得扎实！
